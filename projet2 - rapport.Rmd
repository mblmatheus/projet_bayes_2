---
title: "Salm: extra - Poisson variation in dose - reponse study"
author: BRUNO LOPES Matheus, , TRIOMPHE Amaury
output: pdf_document
date: "15/04/2024"
keep_tex: true    
---

**Lien vers notre Github** : <https://github.com/mblmatheus/projet_bayes_2.git>

# Données étudiées :

```{=tex}
\begin{table}[h]
\centering
\small
\begin{minipage}{0.45\textwidth}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
0    & 10   & 33   & 100  & 333  & 1000 \\\hline
15   & 16   & 16   & 27   & 33   & 20   \\
21   & 18   & 26   & 41   & 38   & 27   \\
29   & 21   & 33   & 69   & 41   & 42   \\
\hline
\end{tabular}
\caption{Dose de quinoléine (mg par plaque)}
\label{tab:tableau1}
\end{minipage}\hfill
\end{table}
```
# Cadre mathématique

## Hypothèses sur nos données ()

## Hypothèses sur nos données ()

Si $\mu_{ij}$ est la moyenne des réversion observées avec une dose de quinoline $i$ sur la plaque $j$, alors il est supposé que le comptage des réversions $y_{ij}$ sur la plaque $j$ avec chaque niveau de dose de quinoline $i$ suit une distribution de Poisson :

$$
y_{ij} \sim Poisson(\mu_{ij})
$$

De plus, la modélisation de la moyenne est effectuée par une fonction logarithmique de la dose $x_i$ avec un ajustement pour traiter la surdispersion, qui est représenté par le terme $\gamma x_i$. En d'autres termes :

$$
  \log{(\mu_{ij})} = \alpha + \beta\log(x_i + 10) + \gamma x_i + \lambda_{ij} \\
  \text{où } \lambda_{ij} \sim \mathcal{N}(0,\tau)
$$

$\alpha, \beta, \gamma, \tau$ ont des priors indépendants "non informatifs" fournis, qui seront supposés comme suit :

$$
  \alpha, \beta, \gamma \sim \mathcal{N}(0, 10^{-6}), \: et \\
  \tau \sim \text{gamma}(10^{-3}, 10^{-3})
$$

Une dernière hypothèse que nous ferons également est que $y_{ij}$ sont indépendants.

$$
y_{ij} \sim Poisson(\mu_{ij})
$$

## Graphe acyclique orienté ()

## Lois conditionnelles ()

Comme nous allons appliquer Hastings-within-Gibbs, nous devrons avoir les lois conditionnelles de tous les paramètres de l'expression de $log(\mu_{ij})$, c'est-à-dire que nous devrons obtenir toutes les lois postérieures. Pour $\alpha$, nous aurons :

$$
  \pi(\alpha|\beta, \gamma, \lambda, y,\tau) \propto \pi(\beta, \gamma, \lambda, y,\tau|\alpha)\pi(\alpha)
$$

Dans le contexte de H-W-Gibbs, comme nous allons mettre à jour les paramètres séparément en considérant les autres comme des valeurs fixes, nous aurons :

$$
    \pi(\alpha|\beta, \gamma, \lambda, y,\tau) \propto \pi(y|\beta, \gamma, \lambda,\tau)\pi(\alpha)\\ =  \pi(\alpha)\prod_{i=1}^{n_{doses}}\prod_{j=1}^{n_{plates}}\pi(y_{ij}|\beta, \gamma, \lambda_{ij},\tau)\\ = \pi(\alpha)\prod_{i=1}^{n_{doses}}\prod_{j=1}^{n_{plates}}\frac{\mu_{ij}^{y_{ij}}}{y_{ij}!}e^{-\mu_{ij}}
$$

Comme tous suivent la même loi a priori, nous aurons des expressions similaires pour $\beta$ et $\gamma$. Pour $\tau$, nous devrons, comme $\tau$ dépend de $\lambda$ qui suit une loi normale, qui dans ce cas est conjuguée par la loi gamma (loi a priori de $\tau$), obtenir directement la loi a posteriori de $\tau$ :

$$
  \tau|\alpha_0, \alpha_{1}, \alpha_{12},\alpha_2, i , b, r \sim gamma(10^{-3} + \frac{n_{doses} + n_{plates}}{2}, 10^{-3} + \frac{\sum_{i=1}^{n_{doses}}\sum_j^{n_{plates}}\lambda_{ij}^2}{2})
$$

Une fois $\tau$ mis à jour dans l'algorithme, nous pourrons mettre à jour chaque $\lambda_{ij}$, pour $i \in \{1, ..., n_{doses}\}$ et $j \in \{1, …, n_{plates}\}$, où chacun aura la loi a posteriori suivante :

$$
  \pi(\lambda_{ij}| \alpha, \beta,\gamma,  y_{ij}, \tau) \propto \pi(\alpha, \beta, \gamma,  y_{ij}, \tau| \lambda_{ij})\pi(\lambda_{ij})
$$

En considérant que $\alpha, \beta, \gamma,\tau$ sont des paramètres déjà fixes et que $\lambda_{ij} \sim N(0,\tau)$, nous pouvons écrire :

$$
  \pi(\lambda_{ij}| \alpha, \beta, \gamma, y_{ij}, \tau) \propto \pi(y_{ij}| \lambda_{ij}, \alpha, \beta, \gamma, \tau)\pi(\lambda_{ij}) \\
  =\frac{\mu_{ij}^{y_{ij}}}{y_{ij}!}\exp{(-\frac{\lambda_{ij}^2}{2\tau}-\mu_{ij})}
$$

Maintenant, ayant toutes les lois conditionnelles, nous pouvons appliquer notre algorithme Hastings-within-Gibbs.

# Résultats de l'implémentation algorithmique ()

```{=tex}
\begin{table}[h]
\centering
\small
\begin{minipage}{0.45\textwidth}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{} &
\multicolumn{2}{|c|}{\textbf{Moyenne}} & \multicolumn{2}{|c|}{\textbf{Écart-type}} \\
\hline
\textbf{Paramètres} & \textbf{Résultat} & \textbf{Énoncé} & \textbf{Résultat} & \textbf{Énoncé} \\
\hline
$\alpha_0$ & -0.5562 & -0.5525 & 0.1865 & 0.1852 \\
$\alpha_1$ & 0.0706 & 0.08382 & 0.3252 & 0.3031 \\
$\alpha_{12}$ & -0.8021 & -0.8165 & 0.4564 & 0.4109 \\
$\alpha_2$ & 1.3511 & 1.346 & 0.2745 & 0.2564 \\
$\sigma$ & 0.3198 & 0.267 & 0.0661 & 0.1471 \\
\hline
\end{tabular}
\caption{Résultats de notre algorithme Hastings within Gibbs}
\end{minipage}
\end{table}
```
-   Allure des chaines de Markov (Matheus)
-   Allure des densités des chaines (Najib)

# Analyse des résultats ()
